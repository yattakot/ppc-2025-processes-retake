# Метод Гаусса – ленточная вертикальная схема

- **Студент** Камалетдинов Рамзан Рамилевич
- **Группа** 3823Б1ПР4
- **Технология** SEQ|MPI
- **Вариант** 16

## 1. Введение

Решение систем линейных алгебраических уравнений является одной из классических задач вычислительной математики.
 Метод Гаусса с выбором главного элемента является классическим алгоритмом для решения СЛАУ.

Цель данной работы: реализация параллельной версии метода Гаусса с использованием технологии MPI и
 вертикальной ленточной схемы распределения данных.
Вертикальная схема предполагает распределение столбцов расширенной матрицы между процессами,
что позволяет эффективно распараллелить процесс исключения Гаусса.

## 2. Постановка задачи

### 2.1 Формальное определение

Дана система линейных алгебраических уравнений вида:

A*x = b

где:

- A — квадратная матрица коэффициентов размером n×n
- x — вектор неизвестных размером n
- b — вектор правых частей размером n

Требуется найти вектор x, удовлетворяющий данной системе.

### 2.2 Формат входных данных

Входные данные представляют собой вектор `std::vector<double>`, где:

- Первый элемент — размер системы n (тип double, но интерпретируется как int)
- Далее следуют элементы расширенной матрицы [A|b], записанные построчно
- Общий размер входного вектора: 1 + n*(n+1)

### 2.3 Формат выходных данных

Выходные данные — вектор `std::vector<double>` размером n, содержащий решение системы уравнений.

### 2.4 Ограничения

- Размер системы n > 0
- Матрица должна быть квадратной
- Размер входного вектора должен соответствовать формуле: 1 + n*(n+1)

## 3. Базовый алгоритм

Последовательный алгоритм метода Гаусса с выбором главного элемента состоит из следующих этапов:

### 3.1 Прямой ход (Forward Elimination)

1. **Выбор главного элемента**: Для каждого столбца k (k = 0..n-1) находится строка
 с максимальным по модулю элементом в столбце k среди строк от k до n-1.

2. **Перестановка строк**: Если найденная строка отличается от текущей, выполняется перестановка строк.

3. **Нормализация ведущей строки**: Все элементы ведущей строки делятся на ведущий элемент
 (диагональный элемент после перестановки).

4. **Исключение переменной**: Для всех строк ниже ведущей выполняется вычитание ведущей строки,
 умноженной на соответствующий коэффициент, чтобы обнулить элементы под диагональю.

### 3.2 Обратный ход (Back Substitution)

Начиная с последней строки, последовательно вычисляются значения неизвестных:

`x[i] = b[i] - (sum)(j=i+1 to n-1) A[i][j] * x[j]`

### 3.3 Особенности реализации

- Используется выбор главного элемента по столбцу для повышения численной устойчивости
- Проверка на близость к нулю ведущего элемента (порог 1e-10)

## 4. Параллельный алгоритм

Параллельный алгоритм метода Гаусса с использованием вертикальной ленточной схемы
 распределения данных реализован с помощью технологии MPI. Основная идея заключается в распределении столбцов
 расширенной матрицы между процессами для параллельного выполнения операций исключения.

### 4.1 Инициализация и распределение данных

1. **Валидация входных данных**: Выполняется только на процессе 0.
 Проверяется корректность размера системы и соответствие размеров входного вектора.

2. **Распределение матрицы**:
   - Процесс 0 извлекает размер системы n из входных данных
   - Размер системы рассылается всем процессам через MPI_Bcast
   - Процесс 0 копирует элементы расширенной матрицы в локальный буфер
   - Полная матрица рассылается всем процессам через MPI_Bcast, обеспечивая каждому процессу полную копию данных

3. **Инициализация решения**: Вектор решения инициализируется нулями на всех процессах

### 4.2 Параллельный прямой ход

Для каждого столбца k (k = 0..n-1) выполняется следующая последовательность:

1. **Поиск главного элемента**: Все процессы синхронно выполняют поиск строки с максимальным по модулю элементом
 в столбце k. Поскольку каждый процесс имеет полную копию матрицы, поиск выполняется одинаково на всех процессах.

2. **Перестановка строк**: Если найденная строка отличается от текущей, все процессы синхронно выполняют
 перестановку строк в своих локальных копиях матрицы.

3. **Нормализация ведущей строки**: Все процессы синхронно делят элементы ведущей строки на ведущий элемент.

4. **Параллельное исключение столбца**:
   - Каждый процесс обрабатывает только свою часть столбцов согласно формуле: start_col = k + rank
   - Процессы обрабатывают столбцы с шагом, равным количеству процессов:
    j = start_col, start_col + size, start_col + 2*size, ...
   - Для каждой строки i (i = k+1..n-1) процесс вычисляет коэффициент
    `factor = A[i][k]` и обновляет только свои столбцы: `A[i][j] -= factor * A[k][j]`

5. **Синхронизация строк**: После локальных вычислений выполняется синхронизация каждой обработанной строки:
   - Не-root процессы отправляют свои обработанные элементы строки процессу 0 через MPI_Send
   - Процесс 0 собирает данные от всех процессов через MPI_Recv, объединяет их в полную строку
   - Процесс 0 рассылает обновленную полную строку всем процессам через MPI_Send
   - Не-root процессы получают обновленную строку через MPI_Recv и обновляют свои локальные копии

### 4.3 Параллельный обратный ход

Обратный ход выполняется последовательно только на процессе 0,
 так как вычисление каждого элемента решения требует знания уже вычисленных значений:

1. **Вычисление решения**: Процесс 0 последовательно вычисляет значения неизвестных, начиная с последней строки:

   `solution[i] = b[i] - sum(j=i+1 to n-1) A[i][j] * solution[j]`

2. **Распространение результата**: После вычисления решения на процессе 0,
 результат рассылается всем процессам через MPI_Bcast, обеспечивая согласованность данных на всех процессах.

### 4.4 Особенности параллельной реализации

- **Дублирование данных**: Каждый процесс хранит полную копию расширенной матрицы,
 что упрощает координацию, но увеличивает требования к памяти
- **Синхронность операций**: Все процессы выполняют синхронные операции поиска главного элемента и перестановки строк,
 что обеспечивает согласованность данных
- **Асинхронность вычислений**: Исключение столбцов выполняется параллельно, но требует последующей синхронизации
- **Координация через процесс 0**: Процесс 0 выступает в роли координатора,
 собирая и распределяя данные между процессами

## 5. Схема распараллеливания

### 4.1 Вертикальная ленточная схема

Вертикальная схема предполагает распределение столбцов расширенной матрицы между процессами.
 Каждый процесс обрабатывает свою часть столбцов согласно формуле:

start_col = k + rank

где k — текущий столбец исключения, rank — номер процесса.

Процессы обрабатывают столбцы с шагом, равным количеству процессов:

j = start_col, start_col + size, start_col + 2*size, ...

### 4.2 Распределение данных

- **Процесс 0**: Хранит полную расширенную матрицу, координирует вычисления
- **Остальные процессы**: Получают копию матрицы через MPI_Bcast, обрабатывают свои столбцы

### 4.3 Схема обмена данными

После выполнения локальных вычислений на каждом процессе выполняется синхронизация строк:

1. **Не-root процессы**: Отправляют свои обработанные элементы строки процессу 0 через MPI_Send
2. **Процесс 0**:
   - Собирает данные от всех процессов через MPI_Recv
   - Обновляет свою матрицу
   - Рассылает обновленную строку всем процессам через MPI_Send
3. **Не-root процессы**: Получают обновленную строку через MPI_Recv и обновляют локальную матрицу

### 4.4 Топология коммуникаций

Процесс 0 (координатор)
    | MPI_Recv (сбор данных)
    | Обновление матрицы
    | MPI_Send (рассылка)
Процессы 1..size-1 (рабочие)

### 4.5 Обратный ход

Обратный ход выполняется только на процессе 0, так как требует последовательного доступа к уже вычисленным значениям.
 Результат затем рассылается всем процессам через MPI_Bcast

### 5.1 Вертикальная ленточная схема распределения

Вертикальная схема предполагает циклическое распределение столбцов между процессами. При обработке столбца k,
 процесс с номером rank обрабатывает столбцы с индексами:

- k + rank
- k + rank + size
- k + rank + 2*size
- и так далее

Это обеспечивает равномерное распределение нагрузки при условии, что количество столбцов
 значительно превышает количество процессов.

### 5.2 Паттерн коммуникаций

Используется звездообразная топология с процессом 0 в центре:

- Все не-root процессы отправляют данные процессу 0
- Процесс 0 собирает и обрабатывает данные
- Процесс 0 рассылает результаты всем процессам

Этот паттерн создает последовательное узкое место на процессе 0, но упрощает координацию
 и обеспечивает согласованность данных.

## 6. Детали реализации

### 5.1 Структура кода

- **common/include/common.hpp**: Определение типов данных (InType, OutType)
- **seq/include/ops_seq.hpp**, **seq/src/ops_seq.cpp**: Последовательная реализация
- **mpi/include/ops_mpi.hpp**, **mpi/src/ops_mpi.cpp**: Параллельная реализация MPI
- **tests/functional/main.cpp**: Функциональные тесты
- **tests/performance/main.cpp**: Тесты производительности

### 5.2 Основные функции MPI реализации

- FindPivotRow(): Поиск строки с максимальным элементом в столбце
- SwapRows(): Перестановка строк матрицы
- EliminateColumn(): Исключение столбца с распределением по процессам
- SynchronizeRow(): Синхронизация строки между процессами
- BackSubstitution(): Обратный ход

### 5.3 Важные замечания

- Все процессы имеют доступ к полной матрице
- Процесс 0 координирует синхронизацию данных
- Размер матрицы достаточен для эффективного распараллеливания

### 5.4 Обработка граничных случаев

- Пустой входной вектор -> возврат false в ValidationImpl
- Отрицательный или нулевой размер системы -> возврат false
- Неправильный размер входного вектора -> возврат false
- Ведущий элемент близок к нулю -> пропуск итерации

### 5.5 Использование памяти

- Каждый процесс хранит полную копию расширенной матрицы: O(n^2) памяти
- Временные буферы для синхронизации: O(n) памяти на процесс
- Общая сложность по памяти: O(n^2) на процесс

## 7. Экспериментальная установка

| Компонент  | Значение                                |
| ---------- | --------------------------------------- |
| CPU        | Apple M1 (8 cores)                      |
| RAM        | 8 GB                                    |
| ОС         | OS: Ubuntu 24.04 (DevContainer / Mac)   |
| Компилятор | GCC 13.3.0 (g++), C++20, CMake, Release |
| MPI        | mpirun (Open MPI) 4.1.6                 |

### 7.1 Тестовые данные

Тестовые данные генерируются в tests/performance/main.cpp:

- Матрица размером 300×300
- Диагонально доминирующая матрица для обеспечения устойчивости
- Ожидаемое решение: вектор из единиц

## 8. Анализ результатов

### 8.1 Корректность

Корректность реализации проверялась следующими способами:

1. **Функциональные тесты**: Реализовано 10 различных тестовых случаев:
   - Системы размером 1×1, 2×2, 3×3, 4×4, 5×5
   - Диагональные матрицы
   - Верхнетреугольные и нижнетреугольные матрицы
   - Матрицы с отрицательными коэффициентами
   - Системы с дробными решениями
   - Системы с нулевой правой частью

2. **Сравнение результатов**: Результаты последовательной и параллельной реализаций сравниваются с точностью до 1e-6.

3. **Проверка граничных случаев**: Тестирование валидации входных данных, обработка некорректных входов.

Все функциональные тесты успешно проходят как для последовательной, так и для параллельной реализации.

### 8.2 Производительность

#### 8.2.1 task_run

| Тип      | Процессы | Время, с  |   Ускорение   | Эффективность |
|----------|----------|-----------|---------------|---------------|
| seq      | 1        | 0.1157    | 1.00          | N/A           |
| mpi      | 1        | 0.0823    | 1.41          | 140.6%        |
| mpi      | 2        | 0.3922    | 0.30          | 14.8%         |
| mpi      | 3        | 0.5861    | 0.20          | 6.6%          |
| mpi      | 4        | 0.9705    | 0.12          | 3.0%          |

#### 8.2.2 pipeline

| Тип      | Процессы | Время, с  |   Ускорение   | Эффективность |
|----------|----------|-----------|---------------|---------------|
| seq      | 1        | 0.1032    | 1.00          | N/A           |
| mpi      | 1        | 0.0847    | 1.22          | 121.9%        |
| mpi      | 2        | 0.4117    | 0.25          | 12.6%         |
| mpi      | 3        | 0.6066    | 0.17          | 5.7%          |
| mpi      | 4        | 0.9289    | 0.11          | 2.8%          |

#### 8.2.3 Дополнительный анализ

Наблюдение: при увеличении количества процессов время выполнения не уменьшается, а увеличивается. Это указывает на то,
 что накладные расходы на коммуникацию между процессами превышают выигрыш от параллельных вычислений.

- **1 процесс MPI**: Показывает лучшее время выполнения (0.0823с для task_run и 0.0847с для pipeline),
 что даже лучше последовательной версии.
  Это может быть связано с оптимизациями компилятора или различиями в реализации.
- **2 процесса**: Время увеличивается по сравнению с 1 процессом
- **3 процесса**: Время продолжает расти
- **4 процесса**: Максимальное время выполнения (0.9705с для task_run), что в несколько раз хуже, чем при 1 процессе

**Масштабируемость алгоритма:**

Алгоритм демонстрирует отрицательную масштабируемость. Основные причины:

1. **Накладные расходы на коммуникацию**: После каждой итерации исключения столбца требуется синхронизация всех строк
 между процессами через MPI_Send/MPI_Recv. Для матрицы 300×300 это означает 300 итераций с синхронизацией,
  что создает значительные задержки.

2. **Дублирование данных**: Каждый процесс хранит полную копию матрицы,
 что неэффективно с точки зрения памяти и не дает преимуществ при распределении вычислений.

3. **Последовательные части алгоритма**: Поиск главного элемента и обратный ход выполняются последовательно,
 что ограничивает параллелизм.

**Эффективность использования процессов:**

Эффективность резко падает с увеличением количества процессов:

- 1 процесс: >100%
- 2 процесса: ~14-15%
- 3 процесса: ~6-7%
- 4 процесса: ~3%

**Узкие места:**

1. **Синхронизация строк**: Функция SynchronizeRow() вызывается для каждой строки после исключения каждого столбца.
 Это создает O(n^2) операций обмена данными.

2. **Последовательная координация**: Процесс 0 должен собирать данные от всех процессов,
 обновлять матрицу и рассылать обратно, что создает последовательное узкое место.

3. **Неоптимальное распределение работы**: Вертикальная схема с шагом rank + k*size может
 создавать неравномерную нагрузку, особенно при малом количестве столбцов на процесс.

**Сравнение режимов task_run и pipeline:**

- **task_run**: Измеряет только время выполнения метода RunImpl()
- **pipeline**: Измеряет время всего пайплайна

В обоих режимах наблюдается одинаковое поведение: ухудшение производительности с увеличением количества процессов.
 Режим pipeline показывает немного лучшее время для 1 процесса (0.0847с против 0.0823с),
  но в целом разница незначительна.

**Выводы по производительности:**

Текущая реализация вертикальной ленточной схемы неэффективна для данной задачи из-за:

- Преобладания накладных расходов на коммуникацию над вычислительной работой
- Последовательных узких мест в алгоритме

## 9. Выводы

### 9.1 Достижения

- Реализована параллельная версия метода Гаусса с использованием вертикальной ленточной схемы
- Обеспечена корректность работы алгоритма на различных тестовых случаях
- Реализована синхронизация данных между процессами

### 9.2 Ограничения и проблемы

- Каждый процесс хранит полную копию матрицы, что ограничивает масштабируемость по памяти
- Синхронизация строк после каждой итерации создает накладные расходы на коммуникацию,
 которые превышают выигрыш от параллельных вычислений
- Обратный ход выполняется последовательно, что ограничивает параллелизм на финальной стадии
- Текущая реализация демонстрирует отрицательную масштабируемость:
 увеличение количества процессов приводит к ухудшению производительности

## 10. Источники

1. Д. Т. Письменный: "Конспект лекций по высшей математике, 19-е издание". АЙРИС ПРЕСС, 2020 год.
2. А. В. Сысоев: Курс лекций по параллельному программированию, ННГУ, 2025 год
3. Документация Open MPI <https://www.open-mpi.org/doc/>
4. Microsoft Функции MPI <https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions>

## Приложение

### Код синхронизации строки между процессами

```cpp
void KamaletdinovRGaussVerticalSchemeMPI::SynchronizeRow(int k, int row, int cols) {
  std::vector<double> row_data(cols - k);
  for (int j = k; j < cols; j++) {
    row_data[j - k] = extended_matrix_[(row * cols) + j];
  }

  if (rank_ == 0) {
    for (int proc = 1; proc < size_; proc++) {
      std::vector<double> recv_data(cols - k);
      MPI_Recv(recv_data.data(), cols - k, MPI_DOUBLE, proc, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      for (int j = k + proc; j < cols; j += size_) {
        row_data[j - k] = recv_data[j - k];
      }
    }
    for (int j = k; j < cols; j++) {
      extended_matrix_[(row * cols) + j] = row_data[j - k];
    }
    for (int proc = 1; proc < size_; proc++) {
      MPI_Send(row_data.data(), cols - k, MPI_DOUBLE, proc, 1, MPI_COMM_WORLD);
    }
  } else {
    MPI_Send(row_data.data(), cols - k, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
    MPI_Recv(row_data.data(), cols - k, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    for (int j = k; j < cols; j++) {
      extended_matrix_[(row * cols) + j] = row_data[j - k];
    }
  }
}
```

### Код исключения столбца

```cpp
void KamaletdinovRGaussVerticalSchemeMPI::EliminateColumn(int k, int cols) {
  double pivot = extended_matrix_[(k * cols) + k];
  if (std::abs(pivot) < 1e-10) {
    return;
  }

  for (int j = k; j < cols; j++) {
    extended_matrix_[(k * cols) + j] /= pivot;
  }

  for (int i = k + 1; i < n_; i++) {
    double factor = extended_matrix_[(i * cols) + k];
    int start_col = k + rank_;
    for (int j = start_col; j < cols; j += size_) {
      extended_matrix_[(i * cols) + j] -= factor * extended_matrix_[(k * cols) + j];
    }
  }

  if (size_ > 1) {
    for (int i = k + 1; i < n_; i++) {
      SynchronizeRow(k, i, cols);
    }
  }
}
```
