# Отчет

- Студент: Куликов Денис Александрович, группа 3823Б1ПР1
- Технология: MPI + SEQ
- Вариант: 11

## 1. Введение

В данной работе реализованы алгоритмы умножения матрицы на вектор.
Рассматриваются две реализации: последовательная (SEQ) и параллельная с
использованием MPI (MPI) с применением ленточной горизонтальной схемы
распараллеливания. Основная цель — изучение подходов к распараллеливанию
задач линейной алгебры и сравнение их производительности.

## 2. Постановка задачи

**Цель работы:**

-Разработать алгоритм умножения матрицы размера M×N на вектор длины N.
-Реализовать последовательную версию для проверки корректности.
-Реализовать параллельную версию с использованием MPI и ленточной
горизонтальной схемы распределения данных.
-Обеспечить корректную балансировку нагрузки при M % P ≠ 0.
-Провести экспериментальное сравнение производительности SEQ и MPI версий.

**Требования к MPI версии:**

-Использовать ленточную горизонтальную схему (разбиение матрицы по строкам).
-Применять MPI_Scatterv для распределения строк матрицы между процессами.
-Применять MPI_Bcast для рассылки вектора всем процессам.
-Применять MPI_Gatherv для сбора результатов на процессе с рангом 0.
-Обеспечить корректную балансировку при неравномерном распределении строк.

## 3. Описание алгоритма (базового/последовательного)

Алгоритм последовательно вычисляет скалярное произведение каждой строки
матрицы на входной вектор. Для матрицы M×N сложность составляет O(M×N).

**Пример реализации:**

```cpp
bool KulikovDMatrixMultiplySEQ::RunImpl() {
  const auto &input = GetInput();
  auto &result = GetOutput();

  for (int i = 0; i < input.rows; i++) {
    int sum = 0;
    for (int j = 0; j < input.cols; j++) {
      sum += input.matrix[(i * input.cols) + j] * input.vector[j];
    }
    result[i] = sum;
  }

  return true;
}
```

## 4. Схема распараллеливания

Параллельный алгоритм использует ленточную горизонтальную схему:
матрица разбивается по строкам между процессами, каждый процесс
считает локальные элементы результирующего вектора, затем результаты
собираются на процессе с рангом 0.

**Пример реализации:**

```cpp
bool KulikovDMatrixMultiplyMPI::RunImpl() {
  const auto &input = GetInput();
  int rank = 0, size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int rows = input.rows;
  int cols = input.cols;

  // 1. Broadcast вектора всем процессам
  std::vector<int> vec(cols);
  if (rank == 0) vec = input.vector;
  MPI_Bcast(vec.data(), cols, MPI_INT, 0, MPI_COMM_WORLD);

  // 2. Расчёт распределения строк
  int base_rows = rows / size;
  int remainder = rows % size;
  int local_rows = base_rows + (rank < remainder ? 1 : 0);

  // 3. Scatterv матрицы
  std::vector<int> local_matrix(local_rows * cols);
  MPI_Scatterv(input.matrix.data(), send_counts.data(), send_displs.data(),
               MPI_INT, local_matrix.data(), local_rows * cols, MPI_INT,
               0, MPI_COMM_WORLD);

  // 4. Локальное умножение
  std::vector<int> local_result(local_rows, 0);
  for (int i = 0; i < local_rows; ++i) {
    int sum = 0;
    for (int j = 0; j < cols; ++j) {
      sum += local_matrix[(i * cols) + j] * vec[j];
    }
    local_result[i] = sum;
  }

  // 5. Gatherv результатов на rank 0
  MPI_Gatherv(local_result.data(), local_rows, MPI_INT,
              GetOutput().data(), recv_counts.data(), displs.data(), MPI_INT,
              0, MPI_COMM_WORLD);

  return true;
}
```

**Пояснение:**

-base_rows = rows / size, remainder = rows % size — балансировка нагрузки.
-MPI_Scatterv — распределение строк матрицы с учётом остатка.
-MPI_Bcast — рассылка вектора (необходим всем процессам).
-MPI_Gatherv — сбор локальных результатов в итоговый вектор.
-Сложность на каждом процессе: O((M/P)×N), где P — число процессов.

## 5. Экспериментальные результаты

### Оценка производительности

-Аппаратное обеспечение и операционная система
    -Процессор: AMD Ryzen 5
    -Оперативная память: 32 ГБ
    -Хост-операционная система: Windows 10
-Инструменты
    -Среда разработки: Visual Studio Code
    -Окружение выполнения: локально
    -Тип сборки: Release
    -MPI: Open MPI / MS-MPI

#### Вывод по результатам

-MPI-реализация показывает ускорение при увеличении числа процессов.
-Для малых размеров матриц (300×300) накладные расходы на коммуникацию
сопоставимы со временем вычислений, поэтому ускорение умеренное.
-При увеличении размера задачи параллельная версия демонстрирует лучшее
масштабирование.
-Балансировка нагрузки при rows % size != 0 реализована корректно,
перекоса нагрузки не наблюдается.
-Параллельная реализация через ленточную горизонтальную схему корректна
и даёт выигрыш по времени по сравнению с последовательной реализацией
при достаточном размере задачи.

## 6. Заключение

-Реализованы последовательная и параллельная версии умножения матрицы на вектор.
-MPI-версия использует ленточную горизонтальную схему с MPI_Scatterv/MPI_Bcast/MPI_Gatherv.
-Корректность проверена с помощью функциональных тестов (1×1, 1×4, 4×1, 3×3, нулевые матрицы).
-Распараллеливание позволяет эффективно уменьшить время вычисления при
увеличении числа процессов и размера задачи.
-Все тесты (функциональные и performance) проходят успешно.
