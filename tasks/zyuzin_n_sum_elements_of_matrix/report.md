# Сумма элементов матрицы

- Студент: Зюзин Никита Михайлович, группа 3823Б1ПР2
- Технологии: SEQ + MPI
- Вариант: 10

## 1. Введение

Задача суммирования элементов матрицы представляет типичную проблему обработки больших объемов численных данных.
С ростом размера матрицы последовательные алгоритмы демонстрируют снижение эффективности.
Параллельная реализация позволит ускорить обработку и обеспечить корректность результатов.

Цель работы - разработать последовательный и параллельный алгоритмы суммирования элементов матрицы и
сравнить их производительность.

## 2. Постановка задачи

Задача: подсчитать сумму всех элементов матрицы.

Входные данные: размеры матрицы (количество строк и столбцов типа `int`), вектор элементов матрицы типа
`std::vector<double>`
Выходные данные: сумма элементов матрицы типа `double`

Ограничения матрицы:

- размеры матрицы должны быть положительными (> 0);
- количество элементов в векторе должно соответствовать произведению количества строк на количество столбцов;
- элементы матрицы представлены в виде чисел типа double.

Ограничения программы:

- должна корректно обрабатывать матрицы различных размеров;
- для параллельной реализации должна обеспечивать корректное распределение данных между процессами и
  объединение результатов.

## 3. Базовый (последовательный) алгоритм (Sequential)

Работа последовательного алгоритма проходит через четыре этапа:

1. `ValidationImpl()` - проверяется, что размеры матрицы положительны и количество элементов
    соответствует произведению размеров.
2. `PreProcessingImpl()` - подготовительный этап, дополнительных операций не выполняется.
3. `RunImpl()` - основной этап обработки матрицы.
   - используется функция `std::accumulate` для суммирования всех элементов вектора;
   - начальное значение аккумулятора устанавливается в 0.0;
   - полученная сумма записывается в выходные данные.
4. `PostProcessingImpl()` - завершающий этап, дополнительных операций не выполняется.

Полноценная реализация последовательного алгоритма представлена в Приложении (п.1).

## 4. Схема параллелизации

Идея параллелизации заключается в том, что матрицу можно разбить на несколько независимых блоков строк и
предоставить обработку каждому блоку отдельному процессу.
Каждый процесс вычисляет частичную сумму своего блока, затем полученные результаты объединяются в итоговый ответ.

Распределение данных:

- Распределение матрицы: исходная матрица делится на n-ое количество примерно равных блоков строк,
  где n равняется числу процессов.
- Распределение нагрузки: для равномерного распределения используются следующие формулы:
  - вычисляется размер блока: количество строк делится (оператор `/`) на количество процессов;
  - вычисляется остаток: количество строк делится (оператор `%`) на количество процессов;
  - первые k процессов (где k - остаток от деления) получают блоки строк увеличенного размера (размер + 1);
  - остальные процессы получают блоки ранее вычисленного размера.
- Определение границ блоков:
  - начальный индекс вычисляется с учетом того, что первые процессы получают блоки увеличенного размера;
  - конечный индекс определяется как начальный индекс, к которому прибавляют количество строк в блоке,
    умноженное на количество столбцов.

Схема связи/топологии:

- Топология: коммуникатор MPI_COMM_WORLD.
- Коммуникационные операции:
  - MPI_Scatterv - операция распределения блоков матрицы между процессами;
  - MPI_Allreduce - операция суммирования частичных результатов (каждый процесс передает свою локальную сумму,
    все процессы одновременно получают итоговый результат).

Ранжирование ролей:
Процесс 0 имеет специальную роль (владеет исходными данными), остальные процессы равноправны:

1. Процесс 0:
   - вычисляет распределение данных между процессами;
   - передает блоки данных остальным процессам используя MPI_Scatterv;
   - обрабатывает свою часть матрицы.
2. Остальные процессы:
   - получают блоки данных от процесса 0;
   - вычисляют местную сумму своего блока;
   - участвуют в коллективной операции MPI_Allreduce для получения глобального результата.

Полноценная реализация распараллеленного алгоритма представлена в Приложении (п.2).

## 5. Детали реализации

Файловая структура:

zyuzin_n_sum_elements_of_matrix/  
├── common/include  
│   └── common.hpp                  # Базовые определения типов  
├── data/  
│   ├── inputs/  
│   │   ├── test1.txt               # Тестовые данные  
│   │   ├── test2.txt  
│   │   ├── ...  
│   │   └── test10.txt  
│   └── outputs/
│       ├── test1.txt               # Ожидаемые результаты  
│       ├── test2.txt  
│       ├── ...  
│       └── test10.txt
├── mpi/  
│   ├── include/ops_mpi.hpp         # MPI версия
│   └── src/ops_mpi.cpp  
├── seq/  
│   ├── include/ops_seq.hpp         # Последовательная версия  
│   └── src/ops_seq.cpp  
└── tests/  
    ├── functional/main.cpp         # Функциональные тесты  
    └── performance/main.cpp        # Производительные тесты  

Ключевые классы:

- ZyuzinNSumElementsOfMatrixSEQ - последовательная реализация.
- ZyuzinNSumElementsOfMatrixMPI - параллельная реализация.

Основные методы:

- ранее описанные (и одинаковые для обеих реализаций):
  - `ValidationImpl()` - проверка входных данных;
  - `PreProcessingImpl()` - подготовительный этап;
  - `PostProcessingImpl()` - завершающий этап;
- `RunImpl()` - основной алгоритм обработки.

Алгоритмические особенности:

- использование `std::accumulate` для последовательного суммирования;
- использование MPI_Scatterv для неравномерного распределения данных;
- использование MPI_Allreduce для получения глобальной суммы;
- корректное вычисление размеров блоков с учетом остатка от деления.

## 6. Экспериментальная среда

Hardware/OS:

- процессор: AMD Ryzen 5 5600x
- ядра/потоки: 6 ядер / 12 потоков
- оперативная память: 16 GB
- операционная система: Ubuntu 25.10
- архитектура: x64

Toolchain:

- компилятор: GCC 15.2.0
- версия: Visual Studio Code 2026
- тип сборки: Release
- система сборки: CMake
- версия MPI: Open MPI 5.0.8

Environment:

- количество процессов: задается через mpirun -n N
- коммуникатор: MPI_COMM_WORLD

Тестовые данные:

1. Функциональные тесты:
   - тестовые данные из файлов (10 тестов с различными размерами матриц)
2. Перформанс тесты:
   - матрица размером 10000 x 10000 элементов
   - общий размер матрицы: 100,000,000 элементов

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность реализации была проверена следующими методами:

- разработано 10 функциональных тестов с заранее известными ожидаемыми результатами;
- тесты охватывают различные сценарии:
  - матрицы различных размеров;
  - матрицы с положительными, отрицательными и нулевыми значениями;
  - матрицы с вещественными числами.

Сравнение последовательной и параллельной версий:

- последовательная версия служит эталоном для проверки параллельной реализации;
- обе версии проходят идентичный набор тестов с одинаковыми входными данными;
- результаты последовательного и MPI алгоритмов полностью совпадают для всех тестовых случаев.

### 7.2 Производительность

Входные данные: матрица 10000×10000

Методы измерений:

- Каждый тест запускается 5 раз
- Берется среднее время выполнения (ΣTime / 5)
- Speedup = Time_seq / Time_mpi
- Efficiency = Speedup / Count * 100%

#### Время выполнения `task_run` (секунды)

| Mode | Count | Time  | Speedup | Efficiency |
|------|-------|-------|---------|------------|
| SEQ  | 1     | 0.949 | 1.00    | —          |
| MPI  | 2     | 0.651 | 1.46    | 73.00 %    |
| MPI  | 4     | 0.529 | 1.79    | 44.75 %    |

#### Время выполнения `pipeline` (секунды)

| Mode | Count | Time  | Speedup | Efficiency |
|------|-------|-------|---------|------------|
| SEQ  | 1     | 0.944 | 1.00    | —          |
| MPI  | 2     | 0.654 | 1.44    | 72.00 %    |
| MPI  | 4     | 0.529 | 1.78    | 44.50 %    |

Анализ результатов:

- Ускорение достигает 1.78 раза на 4 процессах для обоих вариантов реализации, при этом task_run и
  pipeline показывают практически идентичные результаты
- Достаточно высокая эффективность наблюдается на 2 процессах (~73%), что говорит об умеренной
  эффективности распараллеливания при малом числе процессов
- Снижение эффективности на 4 процессах (~45%) происходит из-за накладных расходов MPI, что
  ограничивает масштабируемость для матрицы 10000×10000

## 8. Заключение

В ходе работы была успешно решена задача обработки матрицы размером 10000×10000 с использованием
последовательного алгоритма и технологии MPI для параллельных вычислений в двух вариантах реализации:
task_run и pipeline.

Основные результаты:

- разработаны корректные алгоритмы – созданы последовательная и параллельные версии, прошедшие
  тестирование на корректность результатов;
- реализована схема распараллеливания – выполнено распределение данных между процессами с использованием
  средств MPI, что позволило выполнять вычисления одновременно на нескольких потоках;
- достигнуто ускорение – параллельные реализации демонстрируют ускорение до 1.78 раз на 4 процессах
  по сравнению с последовательной версией. Эффективность при этом составляет около 73% на
  2 процессах и снижается до 45% на 4 процессах, что объясняется накладными расходами MPI.
  Оба варианта (task_run и pipeline) показали практически идентичные результаты, что говорит об
  отсутствии преимущества конвейерной обработки для данной задачи.

## 9. Источники

1. Документация по курсу «Параллельное программирование» // URL:
  <https://learning-process.github.io/parallel_programming_course/ru/index.html>
2. Репозиторий курса «Параллельное программирование» // URL:
  <https://github.com/learning-process/ppc-2025-processes-engineers>
3. Сысоев А. В., Лекции по курсу «Параллельное программирование для кластерных систем».

## Приложение

П.1

```cpp
bool ZyuzinNSumElementsOfMatrixSEQ::RunImpl() {
  const auto &matrix = GetInput();
  GetOutput() = std::accumulate(std::get<2>(matrix).begin(), std::get<2>(matrix).end(), 0.0);
  return true;
}
```

П.2

```cpp
bool ZyuzinNSumElementsOfMatrixMPI::RunImpl() {
  const auto &matrix = GetInput();
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int rows_per_proc = std::get<0>(matrix) / size;
  int ost = std::get<0>(matrix) % size;

  std::vector<int> send_counts(size, 0);
  std::vector<int> start_indexs(size, 0);
  int offset = 0;
  for (int i = 0; i < size; ++i) {
    int rows = rows_per_proc + (ost > 0 ? 1 : 0);
    if (ost > 0) {
      --ost;
    }
    send_counts[i] = rows * std::get<1>(matrix);
    start_indexs[i] = offset;
    offset += send_counts[i];
  }

  std::vector<double> local_data(send_counts[rank]);
  MPI_Scatterv(std::get<2>(matrix).data(), send_counts.data(), start_indexs.data(), MPI_DOUBLE, local_data.data(),
               send_counts[rank], MPI_DOUBLE, 0, MPI_COMM_WORLD);

  double local_sum = std::accumulate(local_data.begin(), local_data.end(), 0.0);
  double global_sum = 0.0;
  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);

  GetOutput() = global_sum;
  return true;
}
```
